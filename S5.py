import re
import pandas as pd
import requests
from transformers import pipeline
import json

# -------------------------
# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• mask-filling
# -------------------------
fill_mask = pipeline("fill-mask", model="xlm-roberta-base", tokenizer="xlm-roberta-base")

# -------------------------
# API Key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö T-NER
# -------------------------
API_KEY = "" # add api

def call_tner(text):
    url = "https://api.aiforthai.in.th/tner"
    headers = {"Apikey": API_KEY}
    try:
        response = requests.post(url, headers=headers, data={"text": text}, timeout=10)

        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if response.status_code == 200:
            try:
                return response.json()
            except ValueError:
                print("‚ùå Response ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà JSON:", response.text[:200])
                return {}
        else:
            print(f"‚ùå API Error {response.status_code}: {response.text[:200]}")
            return {}
    except requests.exceptions.RequestException as e:
        print("‚ùå Request error:", e)
        return {}


# -------------------------
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≤‡∏° POS
# -------------------------
def find_words_by_pos(tner_result, pos_tags):
    words = []
    tokens = tner_result.get("words", [])
    pos = tner_result.get("POS", [])
    for idx, (w, p) in enumerate(zip(tokens, pos)):
        if p in pos_tags:
            words.append((idx, w))
    return words

# -------------------------
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô fill-mask
# -------------------------
def check_word_with_fill_mask(sentence, target_word):
    masked_sentence = sentence.replace(target_word, fill_mask.tokenizer.mask_token, 1)
    preds = fill_mask(masked_sentence)
    predicted_tokens = [p["token_str"].strip() for p in preds]
    return True, predicted_tokens

# -------------------------
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô normalize (‡πÄ‡∏≠‡∏≤ whitespace ‡πÅ‡∏•‡∏∞ newlines ‡∏≠‡∏≠‡∏Å)
# -------------------------
def normalize_word(w):
    return w.replace("\n", "").replace("\r", "").replace(" ", "").lower()

# -------------------------
# ‡πÇ‡∏´‡∏•‡∏î dataset ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô set
# -------------------------
spoken_words_dataset = pd.read_csv("/content/speak_words(in) (1).csv")["word"].dropna().tolist()
notinlan_dataset = pd.read_csv("/content/notinlan_words.csv")["notinlan"].dropna().tolist()
local_words_context = pd.read_csv("/content/sample_local_dialect  (1)(in).csv")["local_word"].dropna().tolist()

spoken_words_set = set(normalize_word(w) for w in spoken_words_dataset)
notinlan_set = set(normalize_word(w) for w in notinlan_dataset)
local_dialect_set = set(normalize_word(w) for w in local_words_context)

# -------------------------
# keyword_dict ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
# -------------------------
keyword_dict = {
      "conjunctions": ["‡∏à‡∏ô", "‡πÅ‡∏°‡πâ‡∏°‡∏µ", "‡πÅ‡∏°‡πâ‡∏ß‡πà‡∏≤", "‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ", "‡∏à‡∏∂‡∏á‡∏°‡∏µ", "‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤", "‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô", "‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö", "‡∏£‡∏∞‡∏ß‡∏±‡∏á", "‡∏´‡∏≤‡∏Å‡πÉ‡∏ä‡∏™‡∏∑‡∏≠", "‡∏ó‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ",
                       "‡∏´‡∏≤‡∏Å‡πÉ‡∏ä‡πâ", "‡πÅ‡∏ï‡πà‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏°", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏°‡πà", "‡∏´‡∏£‡∏∑‡∏≠", "‡∏ñ‡∏∂‡∏á‡πÅ‡∏°‡πâ", "‡∏¢‡∏±‡∏á‡∏°‡∏µ", "‡∏Å‡∏≤‡∏£", "‡πÄ‡∏û‡∏£‡∏≤‡∏∞", "‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô"],

      "prepositions": ["‡πÉ‡∏ô‡∏Å‡∏≤‡∏£", "‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ", "‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö", "‡∏î‡πâ‡∏ß‡∏¢", "‡∏Ç‡∏≠‡∏á‡∏™‡∏±‡∏á‡∏Ñ‡∏°", "‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡∏ï‡πà‡∏≠‡∏™‡∏±‡∏á‡∏Ñ‡∏°", "‡∏¢‡∏±‡∏á", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß",
                       "‡∏Å‡πá‡∏à‡∏∞", "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏ö‡∏ß‡πà‡∏≤", "‡πÅ‡∏Å‡πà‡∏™‡∏±‡∏á‡∏Ñ‡∏°", "‡πÅ‡∏Å‡πà‡∏ú‡∏π‡πâ‡∏≠‡∏∑‡πà‡∏ô", "‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß", "‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ô‡∏ö‡∏≤‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°", "‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ï‡∏ô", "‡∏™‡∏∑‡πà‡∏≠",
                       "‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å", "‡∏ó‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô", "‡∏ï‡πà‡∏≠‡∏á"],

      "classifiers": ["‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á"]
}

# -------------------------
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
# -------------------------
def evaluate_student_text(student_text, keyword_dict,
                          spoken_words_set,
                          notinlan_set,
                          local_dialect_set,
                          full_score=1.0,
                          deduct_per_word=0.5):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
    - ‡∏ï‡∏£‡∏ß‡∏à POS (conjunctions, prepositions, classifiers) + fill-mask
    - ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏û‡∏π‡∏î / ‡∏Ñ‡∏≥‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤ / ‡∏Ñ‡∏≥‡∏ñ‡∏¥‡πà‡∏ô
    - ‡∏Ñ‡∏∑‡∏ô errors ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
    """
    errors = {k: [] for k in (list(keyword_dict.keys()) + ["slang", "dialect", "invalid_word"])}
    total_wrong = 0
    penalty = 0.0

    # 1Ô∏è‚É£ ‡∏ï‡∏£‡∏ß‡∏à POS + fill-mask
    tner_result = call_tner(student_text)
    words_list = [w.strip() for w in re.split(r'\s+', student_text.strip()) if w.strip()]
    pos_categories = {
        "conjunctions": ["CNJ"],
        "prepositions": ["P"],
        "classifiers": ["CL"]
    }

    for key, pos_list in pos_categories.items():
        words = find_words_by_pos(tner_result, pos_list)
        for idx, word in words:
            prev_word = words_list[idx-1] if 0 <= idx-1 < len(words_list) else ""
            next_word = words_list[idx+1] if 0 <= idx+1 < len(words_list) else ""

            is_valid, preds = check_word_with_fill_mask(student_text, word)

            # normalize word ‡πÄ‡∏≠‡∏≤ \n ‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏£‡∏ß‡∏à keyword
            clean_word = normalize_word(word)
            clean_prev = normalize_word(prev_word)
            clean_next = normalize_word(next_word)

            matched_keyword = None
            for kw in keyword_dict.get(key, []):
                if kw.startswith(clean_prev + clean_word + clean_next) or clean_word in kw:
                    matched_keyword = kw
                    break

            is_wrong = False
            if preds:
                if not matched_keyword and (clean_word not in preds):
                    is_wrong = True

            if is_wrong:
                total_wrong += 1

            errors[key].append({
                "word": clean_word,
                "predicted": preds,
                "prev_word": prev_word,
                "next_word": next_word,
                "matched_keyword": matched_keyword,
                "is_wrong": is_wrong
            })

    # 2Ô∏è‚É£ ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏û‡∏π‡∏î / ‡∏Ñ‡∏≥‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤ / ‡∏Ñ‡∏≥‡∏ñ‡∏¥‡πà‡∏ô
    clean_text = normalize_word(student_text)

    # ‡∏†‡∏≤‡∏©‡∏≤‡∏û‡∏π‡∏î
    spoken_words_found = [w for w in spoken_words_set if w in clean_text]
    for w in spoken_words_found:
        errors["slang"].append({"word": w, "is_wrong": True})
        penalty += deduct_per_word

    # ‡∏Ñ‡∏≥‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤
    notinlan_found = [w for w in notinlan_set if w in clean_text]
    for w in notinlan_found:
        errors["invalid_word"].append({"word": w, "is_wrong": True})
        penalty += deduct_per_word

    # ‡∏Ñ‡∏≥‡∏ñ‡∏¥‡πà‡∏ô
    dialect_found = [w for w in local_dialect_set if w in clean_text]
    for w in dialect_found:
        errors["dialect"].append({"word": w, "is_wrong": True})
        penalty += deduct_per_word

    # 3Ô∏è‚É£ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°
    score = full_score - 0.5 * total_wrong - penalty
    score = max(min(score, full_score), 0.0)

    return {
        "errors": errors,
        "score": round(score,2)
    }

# -------------------------
# üî• ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
# -------------------------
if __name__ == "__main__":
    student_answer = """‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏á‡∏Ñ‡∏°(Social Media)‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏Ñ‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ ‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå ‡∏´‡∏£‡∏∑‡∏≠ ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏á‡∏Ñ‡∏°
‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏û‡∏£‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
‡∏Å‡πá‡∏ï‡∏≤‡∏°‡∏´‡∏≤‡∏Å‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏°‡∏±‡∏î‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡∏ï‡πà‡∏≠‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏£‡∏ß‡∏°
‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ú‡∏π‡πâ‡∏Ñ‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏°‡πà‡∏ô‡πâ‡∏≠‡∏¢‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ä‡πâ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ï‡∏•‡∏≤‡∏î‡∏ó‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏ó‡∏≤‡∏á
‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à ‡πÅ‡∏ï‡πà‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ñ‡∏ô‡πÉ‡∏ô‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏Å‡πá‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡∏ô‡πÄ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏´‡∏•‡∏≠‡∏Å‡∏•‡∏ß‡∏á‡∏à‡∏≤‡∏Å‡∏ö‡∏≤‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°"""

    result = evaluate_student_text(student_answer, keyword_dict,
                                   spoken_words_set,
                                   notinlan_set,
                                   local_dialect_set)
    print(json.dumps(result, ensure_ascii=False, indent=2))
