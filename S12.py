import re
import pandas as pd
import requests
from transformers import pipeline
import json

# -------------------------
# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• mask-filling
# -------------------------
fill_mask = pipeline("fill-mask", model="xlm-roberta-base", tokenizer="xlm-roberta-base")

# -------------------------
# API Key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö T-NER
# -------------------------
API_KEY = "" # add api

def call_tner(text):
    url = "https://api.aiforthai.in.th/tner"
    headers = {"Apikey": API_KEY}
    try:
        response = requests.post(url, headers=headers, data={"text": text}, timeout=10)

        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if response.status_code == 200:
            try:
                return response.json()
            except ValueError:
                print("‚ùå Response ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà JSON:", response.text[:200])
                return {}
        else:
            print(f"‚ùå API Error {response.status_code}: {response.text[:200]}")
            return {}
    except requests.exceptions.RequestException as e:
        print("‚ùå Request error:", e)
        return {}


# -------------------------
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≤‡∏° POS
# -------------------------
def find_words_by_pos(tner_result, pos_tags):
    words = []
    tokens = tner_result.get("words", [])
    pos = tner_result.get("POS", [])
    for idx, (w, p) in enumerate(zip(tokens, pos)):
        if p in pos_tags:
            words.append((idx, w))
    return words

# -------------------------
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô fill-mask
# -------------------------
def check_word_with_fill_mask(sentence, target_word):
    masked_sentence = sentence.replace(target_word, fill_mask.tokenizer.mask_token, 1)
    preds = fill_mask(masked_sentence)
    predicted_tokens = [p["token_str"].strip() for p in preds]
    return True, predicted_tokens

# -------------------------
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô normalize (‡πÄ‡∏≠‡∏≤ whitespace ‡πÅ‡∏•‡∏∞ newlines ‡∏≠‡∏≠‡∏Å)
# -------------------------
def normalize_word(w):
    return w.replace("\n", "").replace("\r", "").replace(" ", "").lower()

# -------------------------
# ‡πÇ‡∏´‡∏•‡∏î dataset ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô set
# -------------------------
spoken_words_dataset = pd.read_csv("/content/dataset_speak_word(in).csv")["word"].dropna().tolist()
notinlan_dataset = pd.read_csv("/content/dataset_notinlan_words(in).csv")["notinlan"].dropna().tolist()
local_words_context = pd.read_csv("/content/S12_sample_local_dialect  (1)(in)(in).csv")["local_word"].dropna().tolist()

spoken_words_set = set(normalize_word(w) for w in spoken_words_dataset)
notinlan_set = set(normalize_word(w) for w in notinlan_dataset)
local_dialect_set = set(normalize_word(w) for w in local_words_context)

# -------------------------
# keyword_dict ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
# -------------------------
keyword_dict = {
      "conjunctions": ["‡∏à‡∏ô", "‡πÅ‡∏°‡πâ‡∏°‡∏µ", "‡πÅ‡∏°‡πâ‡∏ß‡πà‡∏≤", "‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ", "‡∏à‡∏∂‡∏á‡∏°‡∏µ", "‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤", "‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô", "‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö", "‡∏£‡∏∞‡∏ß‡∏±‡∏á", "‡∏ß‡∏±‡∏ô‡∏ï‡πà‡∏≠‡∏°‡∏≤", "‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏†‡∏±‡∏¢",
                       "‡∏´‡∏≤‡∏Å‡πÉ‡∏ä‡πâ", "‡πÅ‡∏ï‡πà‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏°", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏°‡πà", "‡∏´‡∏£‡∏∑‡∏≠", "‡∏ñ‡∏∂‡∏á‡πÅ‡∏°‡πâ", "‡∏¢‡∏±‡∏á‡∏°‡∏µ", "‡∏Å‡∏≤‡∏£", "‡πÄ‡∏û‡∏£‡∏≤‡∏∞",
                       "‡πÉ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ", "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô", "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°", "‡πÅ‡∏°‡πâ‡πÅ‡∏ï‡πà", "‡∏ï‡∏•‡∏≠‡∏î‡∏à‡∏ô", "‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏Å", "‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô", "‡∏™‡πà‡∏ß‡∏ô", "‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•", "‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á",
                       "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£", "‡∏ó‡∏≥‡πÉ‡∏´‡πâ", "‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô", "‡πÇ‡∏ó‡∏©‡πÅ‡∏Å‡πà‡∏™‡∏±‡∏á‡∏Ñ‡∏°", "‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏ô", "‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤",
                       "‡∏™‡∏∞‡∏î‡∏ß‡∏Å", "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ", "‡∏£‡∏∞‡∏î‡∏°", "‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á", "‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô", "‡πÑ‡∏õ‡πÄ‡∏à‡∏≠", "‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô", "‡∏Ñ‡πâ‡∏≤‡∏Ç‡∏≤‡∏¢",
                       "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á", "‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏¥"],

      "prepositions": ["‡πÉ‡∏ô‡∏Å‡∏≤‡∏£", "‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ", "‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö", "‡∏î‡πâ‡∏ß‡∏¢", "‡∏Ç‡∏≠‡∏á‡∏™‡∏±‡∏á‡∏Ñ‡∏°", "‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡∏ï‡πà‡∏≠‡∏™‡∏±‡∏á‡∏Ñ‡∏°", "‡∏¢‡∏±‡∏á", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß",
                       "‡∏Å‡πá‡∏à‡∏∞", "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏ö‡∏ß‡πà‡∏≤", "‡πÅ‡∏Å‡πà‡∏™‡∏±‡∏á‡∏Ñ‡∏°", "‡πÅ‡∏Å‡πà‡∏ú‡∏π‡πâ‡∏≠‡∏∑‡πà‡∏ô", "‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß", "‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ô‡∏ö‡∏≤‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°", "‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ï‡∏ô", "‡∏™‡∏∑‡πà‡∏≠",
                       "‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å", "‡∏ó‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô", "‡∏ï‡πà‡∏≠‡∏á", "‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á", "‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö", "‡∏û‡∏≠", "‡πÇ‡∏£‡∏á‡∏á‡∏≤‡∏ô", "‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô", "‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏ó‡∏©",
                       "‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠", "‡πÄ‡∏î‡∏∑‡∏≠‡∏î‡∏£‡πâ‡∏≠‡∏ô", "‡∏°‡∏µ‡∏Å‡∏≤‡∏£", "‡∏≠‡∏µ‡∏Å‡∏î‡πâ‡∏ß‡∏¢", "‡∏ä‡∏ß‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠", "‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏•‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠", "‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á",
                       "‡πÅ‡∏ï‡πà‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏ô", "‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô", "‡∏™‡∏∞‡∏î‡∏ß‡∏Å", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£", "‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ú‡πà‡∏≤‡∏ô", "‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö",
                       "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ï‡πà‡∏≤‡∏á‡πÜ", "‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢", "‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á", "‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å", "‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢", "‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£",
                       "‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô", "‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà", "‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á", "‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£", "‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£"],

      "classifiers": ["‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á", "‡∏ö‡∏≤‡∏á‡∏Ñ‡∏ô", "‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô", "‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å", "‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç", "‡πÑ‡∏°‡πà‡∏î‡∏µ", "‡∏≠‡∏≤‡∏à‡∏à‡∏∞",
                      "‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô", "‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à‡∏°‡∏≤‡∏Å", "‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á", "‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢"]
}

# -------------------------
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
# -------------------------
def evaluate_student_text(student_text, keyword_dict,
                          spoken_words_set,
                          notinlan_set,
                          local_dialect_set,
                          full_score=2.0,
                          deduct_per_word=0.5):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
    - ‡∏ï‡∏£‡∏ß‡∏à POS (conjunctions, prepositions, classifiers) + fill-mask
    - ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏û‡∏π‡∏î / ‡∏Ñ‡∏≥‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤ / ‡∏Ñ‡∏≥‡∏ñ‡∏¥‡πà‡∏ô
    - ‡∏Ñ‡∏∑‡∏ô errors ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
    """
    errors = {k: [] for k in (list(keyword_dict.keys()) + ["slang", "dialect", "invalid_word"])}
    total_wrong = 0
    penalty = 0.0

    # 1Ô∏è‚É£ ‡∏ï‡∏£‡∏ß‡∏à POS + fill-mask
    tner_result = call_tner(student_text)
    words_list = [w.strip() for w in re.split(r'\s+', student_text.strip()) if w.strip()]
    pos_categories = {
        "conjunctions": ["CNJ"],
        "prepositions": ["P"],
        "classifiers": ["CL"]
    }

    for key, pos_list in pos_categories.items():
        words = find_words_by_pos(tner_result, pos_list)
        for idx, word in words:
            prev_word = words_list[idx-1] if 0 <= idx-1 < len(words_list) else ""
            next_word = words_list[idx+1] if 0 <= idx+1 < len(words_list) else ""

            is_valid, preds = check_word_with_fill_mask(student_text, word)

            # normalize word ‡πÄ‡∏≠‡∏≤ \n ‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏£‡∏ß‡∏à keyword
            clean_word = normalize_word(word)
            clean_prev = normalize_word(prev_word)
            clean_next = normalize_word(next_word)

            matched_keyword = None
            for kw in keyword_dict.get(key, []):
                if kw.startswith(clean_prev + clean_word + clean_next) or clean_word in kw:
                    matched_keyword = kw
                    break

            is_wrong = False
            if preds:
                if not matched_keyword and (clean_word not in preds):
                    is_wrong = True

            if is_wrong:
                total_wrong += 1

            errors[key].append({
                "word": clean_word,
                "predicted": preds,
                "prev_word": prev_word,
                "next_word": next_word,
                "matched_keyword": matched_keyword,
                "is_wrong": is_wrong
            })

    # 2Ô∏è‚É£ ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏û‡∏π‡∏î / ‡∏Ñ‡∏≥‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤ / ‡∏Ñ‡∏≥‡∏ñ‡∏¥‡πà‡∏ô
    clean_text = normalize_word(student_text)

    # ‡∏†‡∏≤‡∏©‡∏≤‡∏û‡∏π‡∏î
    spoken_words_found = [w for w in spoken_words_set if w in clean_text]
    for w in spoken_words_found:
        errors["slang"].append({"word": w, "is_wrong": True})
        penalty += deduct_per_word

    # ‡∏Ñ‡∏≥‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤
    notinlan_found = [w for w in notinlan_set if w in clean_text]
    for w in notinlan_found:
        errors["invalid_word"].append({"word": w, "is_wrong": True})
        penalty += deduct_per_word

    # ‡∏Ñ‡∏≥‡∏ñ‡∏¥‡πà‡∏ô
    dialect_found = [w for w in local_dialect_set if w in clean_text]
    for w in dialect_found:
        errors["dialect"].append({"word": w, "is_wrong": True})
        penalty += deduct_per_word

    # 3Ô∏è‚É£ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°
    score = full_score - 0.5 * total_wrong - penalty
    score = max(min(score, full_score), 0.0)

    return {
        "errors": errors,
        "score": round(score,2)
    }

# -------------------------
# üî• ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
# -------------------------
if __name__ == "__main__":
    student_answer = """‡πÄ‡∏´‡πá‡∏ô‡∏î‡πâ‡∏ß‡∏¢ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏¢‡∏∏‡∏Ñ‡∏™‡∏°‡∏±‡∏¢‡∏ô‡∏µ‡πâ‡∏•‡πâ‡∏ß‡∏ô‡∏°‡∏µ‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏õ‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏∑‡πà‡∏≠‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏≠‡∏∑‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡πå
‡πÉ‡∏ä‡πâ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡πà‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏ö‡∏≤‡∏á‡∏Ñ‡∏ô‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà
‡∏ñ‡∏π‡∏Å‡πÉ‡∏à‡∏Å‡πá‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢ ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡πå‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏î‡∏µ ‡∏Ñ‡∏∑‡∏≠ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏£‡πâ‡∏≤‡∏¢
‡∏Å‡∏±‡∏ô‡∏ó‡∏≤‡∏á‡∏≠‡πâ‡∏≠‡∏° ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏∞‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á"""

    result = evaluate_student_text(student_answer, keyword_dict,
                                   spoken_words_set,
                                   notinlan_set,
                                   local_dialect_set)
    print(json.dumps(result, ensure_ascii=False, indent=2))
